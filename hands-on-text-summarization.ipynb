{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Hands-On Text Summarization with Pretrained Models üìùü§ñ**","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction üìò\nWelcome to this hands-on guide to **Text Summarization**! \n\nIn this notebook, we will explore how to automatically summarize text using state-of-the-art **Pretrained Transformer Models** from Hugging Face. You will learn how to load these models, process text, and generate concise summaries for news articles or stories.\n\n**By the end of this notebook, you will understand:**\n*   What pretrained models and transformers are.\n*   The difference between extractive and abstractive summarization.\n*   How to use libraries like `transformers` to perform NLP tasks.\n*   How to implement and compare different models like **BART**, **Pegasus**, and **mT5**.","metadata":{}},{"cell_type":"markdown","source":"---\n## 2. What Are Pretrained Models? üß†\n\n### **The Era of Transfer Learning**\nIn the past, training a Neural Network for NLP required massive datasets and weeks of computing time. Today, we use **Transfer Learning**:\n1.  **Pretraining**: A model is trained on a huge amount of text (like the entire internet) to learn the general rules of language (grammar, context, facts).\n2.  **Fine-Tuning (or Inference)**: We take this \"smart\" model and use it for a specific task, like summarization.\n\n### **Why Transformers?**\n*   **Context Awareness**: Unlike older models (RNNs), Transformers can look at the *entire* sentence at once using a mechanism called **Self-Attention**.\n*   **Parallel Processing**: They are faster to train and run.\n*   **State-of-the-Art**: Models like BERT, GPT, and T5 are all based on the Transformer architecture.","metadata":{}},{"cell_type":"markdown","source":"---\n## 3. What Is Text Summarization? ‚ú®\n\nText summarization is the task of shortening a text while keeping its main ideas. There are two main types:\n\n| Type | Description | Analogy |\n| :--- | :--- | :--- |\n| **Extractive** | Selects and copies the most important sentences directly from the text. | Like highlighting key sentences with a marker. |\n| **Abstractive** | Generates new sentences to rephrase the core meaning. It can use words not present in the original text. | Like a human rewriting a summary in their own words. |\n\n**In this notebook, we will focus on Abstractive Summarization**, which is more powerful but harder to achieve than extractive methods. Transformers excel at this!\n","metadata":{}},{"cell_type":"markdown","source":"---\n## 4. Hugging Face & Pretrained Models üöÄ\n\n[Hugging Face](https://huggingface.co/) is the \"GitHub of Machine Learning\". It provides:\n*   **The Hub**: A repository of thousands of pretrained models shared by the community (Google, Facebook, Microsoft, etc.).\n*   **`transformers` Library**: A Python package that makes it incredibly easy to download and use these models.\n\nWe will be using three famous models today:\n1.  **BART** (Facebook)\n2.  **Pegasus** (Google)\n3.  **mT5** (Multilingual T5)","metadata":{}},{"cell_type":"markdown","source":"---\n## 5. Notebook Walkthrough\n### üõ†Ô∏è Step 1: Install Dependencies\nWe need to install the `transformers` library to access the models and `torch` (PyTorch) as the underlying Deep Learning framework.","metadata":{}},{"cell_type":"code","source":"#!pip install transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:04:05.231577Z","iopub.execute_input":"2026-01-07T16:04:05.232649Z","iopub.status.idle":"2026-01-07T16:04:05.239378Z","shell.execute_reply.started":"2026-01-07T16:04:05.232557Z","shell.execute_reply":"2026-01-07T16:04:05.238176Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### üìù Step 2: Define the Source Text\nHere, we define a sample text that we want to summarize. This article talks about cultural differences in dining etiquette between the US and Denmark.","metadata":{}},{"cell_type":"code","source":"text = \"\"\"\nWhen Brooke Black and her Danish husband first lived together in the United States, she doesn‚Äôt recall their different dining habits ever really being a thing. It wasn‚Äôt until the 44-year-old mother of two moved to Denmark in 2020 that she became acutely aware that she didn‚Äôt use eating utensils like her husband ‚Äî or pretty much any of the Europeans around her.\nGrowing up in Illinois, Black says her mother only set their family dinner table with forks, unless there was something being served, such as steak, that warranted a knife to cut it.\n‚ÄúI have not used a knife my whole life,‚Äù says Black, who shares cultural commentary about her daily life in Denmark on her Instagram account. While she jokes that she ‚Äústands by that a fork can also be a knife,‚Äù she never learned to eat in the ‚Äúzigzagging‚Äù manner of many Americans who will cut meat with the knife in their dominant hand before switching the fork back into that one to eat.\nBut in Denmark at family gatherings, with her fork held in her right hand from the get-go ‚Äî tines up ‚Äî and her knife largely untouched beside the plate, Black soon realized she stuck out.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:04:05.247498Z","iopub.execute_input":"2026-01-07T16:04:05.247846Z","iopub.status.idle":"2026-01-07T16:04:05.257442Z","shell.execute_reply.started":"2026-01-07T16:04:05.247814Z","shell.execute_reply":"2026-01-07T16:04:05.255432Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"---\n## 6. Models Used in This Notebook\n\n### üîπ Model 1: BART (Facebook)\n**BART (Bidirectional and Auto-Regressive Transformers)** is a model designed for text generation. \n*   **Architecture**: It combines the bidirectional encoder of BERT (good for understanding) with the autoregressive decoder of GPT (good for writing).\n*   **Best For**: Summarization and translation.\n*   **Why use it?**: It is robust and produces coherent, fluent English summaries.","metadata":{}},{"cell_type":"markdown","source":"### ‚öôÔ∏è Step 3: Load BART Model & Tokenizer\nWe load the `facebook/bart-large-cnn` model, which has been specifically fine-tuned on the CNN/Daily Mail dataset for news summarization.\n*   **Tokenizer**: Converts text into numbers (tokens) the model can understand.\n*   **Model**: The brain that performs the summarization.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nBERT = \"facebook/bart-large-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(BERT)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(BERT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:04:05.275597Z","iopub.execute_input":"2026-01-07T16:04:05.275964Z","iopub.status.idle":"2026-01-07T16:04:53.600816Z","shell.execute_reply.started":"2026-01-07T16:04:05.275939Z","shell.execute_reply":"2026-01-07T16:04:53.599070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401c53ff8e45458cb31b6dfdd777ba47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44233a9d0a7465dbf481e2517b1a142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f16c18a2baa4db78d281c237e079539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fef719509e74e3dac648c8be8ba42ab"}},"metadata":{}},{"name":"stderr","text":"2026-01-07 16:04:28.248266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767801868.521418      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767801868.599296      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767801869.270739      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767801869.270807      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767801869.270809      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767801869.270812      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7542afc580e440df8482037c86a637e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7baae58f09414996f0a122a776052b"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### üß™ Step 4: Create the Summarization Function\nWe define a function `summarize` to handle the generation process.\n*   **`tokenizer(...)`**: Prepares the text. `truncation=True` ensures we don't exceed the model's limit.\n*   **`model.generate(...)`**:\n    *   `num_beams=4`: Uses \"Beam Search\" to find the best possible sentence path, not just the first good word.\n    *   `no_repeat_ngram_size=2`: Prevents the model from repeating the same 2-word phrases.\n    *   `max_length`: Limits the summary size.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef summarize(text, max_input_len=512, max_summary_len=100):\n    text = re.sub(r'\\s+', ' ', text.strip())  # clean spaces and newlines\n    inputs = tokenizer(\n        text, return_tensors=\"pt\", truncation=True, max_length=max_input_len\n    )\n    output_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_summary_len,\n        num_beams=4,\n        no_repeat_ngram_size=2\n    )\n    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:04:53.602998Z","iopub.execute_input":"2026-01-07T16:04:53.604167Z","iopub.status.idle":"2026-01-07T16:04:53.610353Z","shell.execute_reply.started":"2026-01-07T16:04:53.604100Z","shell.execute_reply":"2026-01-07T16:04:53.609512Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### üöÄ Step 5: Run BART Summarization\nLet's see what BART thinks is important in our text!","metadata":{}},{"cell_type":"code","source":"summary = summarize(text)\nprint(\"üìÑ Summary:\\n\", summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:04:53.612171Z","iopub.execute_input":"2026-01-07T16:04:53.613184Z","iopub.status.idle":"2026-01-07T16:05:13.685764Z","shell.execute_reply.started":"2026-01-07T16:04:53.613120Z","shell.execute_reply":"2026-01-07T16:05:13.684000Z"}},"outputs":[{"name":"stdout","text":"üìÑ Summary:\n Brooke Black moved to Denmark with her husband in 2020. She quickly realized she didn't use eating utensils like other Europeans. The 44-year-old mother of two shares cultural commentary about her daily life in Denmark on her Instagram account, @brookeblackdanes.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"---\n### üîπ Model 2: Pegasus (Google)\n**Pegasus (Pre-training with Extracted Gap-sentences for Abstractive SUmmarization)** was designed explicitly for summarization.\n*   **Unique Training**: During training, entire important sentences were removed from documents, and the model had to generate them.\n*   **Strength**: It typically achieves higher performance on summarization benchmarks compared to general-purpose models.","metadata":{}},{"cell_type":"markdown","source":"### ‚öôÔ∏è Step 6: Load Pegasus Model\nWe are using `google/pegasus-cnn_dailymail`, also fine-tuned for news.","metadata":{}},{"cell_type":"code","source":"model_name_pegasus = \"google/pegasus-cnn_dailymail\"\ntokenizer_pegasus = AutoTokenizer.from_pretrained(model_name_pegasus)\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_name_pegasus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:05:13.688007Z","iopub.execute_input":"2026-01-07T16:05:13.688325Z","iopub.status.idle":"2026-01-07T16:05:50.988332Z","shell.execute_reply.started":"2026-01-07T16:05:13.688297Z","shell.execute_reply":"2026-01-07T16:05:50.985718Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065bad7b347c46e5ae6598a95bff3d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d86766aac0411e9f8e23322852d977"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b918937dbb42a792ec8cb86d70efc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3515bacb85714bd693b53c3b56871256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb1e4f9871b4728ad553f0ef5a320d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4047b1428e5f4875b31b715900175c7b"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1422c9cf734e9f9fdf8db782eaaf49"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### üß™ Step 7: Define Pegasus Function\nSimilar to the BART function, but using the Pegasus tokenizer and model instances.","metadata":{}},{"cell_type":"code","source":"def summarize_pegasus(text, max_input_len=512, max_summary_len=100):\n    import re\n    text = re.sub(r'\\s+', ' ', text.strip())\n    inputs = tokenizer_pegasus(text, return_tensors=\"pt\", truncation=True, max_length=max_input_len)\n    output_ids = model_pegasus.generate(inputs[\"input_ids\"], max_length=max_summary_len, num_beams=4, no_repeat_ngram_size=2)\n    summary = tokenizer_pegasus.decode(output_ids[0], skip_special_tokens=True)\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:05:50.991245Z","iopub.execute_input":"2026-01-07T16:05:50.991626Z","iopub.status.idle":"2026-01-07T16:05:51.004084Z","shell.execute_reply.started":"2026-01-07T16:05:50.991601Z","shell.execute_reply":"2026-01-07T16:05:51.002246Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### üöÄ Step 8: Run Pegasus Summarization\nCompare this result with BART. Is it more concise? Does it capture different details?","metadata":{}},{"cell_type":"code","source":"# Example\nprint(\"üìÑ PEGASUS Summary:\\n\", summarize_pegasus(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:05:51.007254Z","iopub.execute_input":"2026-01-07T16:05:51.008725Z","iopub.status.idle":"2026-01-07T16:06:52.492499Z","shell.execute_reply.started":"2026-01-07T16:05:51.008675Z","shell.execute_reply":"2026-01-07T16:06:52.491261Z"}},"outputs":[{"name":"stdout","text":"üìÑ PEGASUS Summary:\n Brooke Black and her Danish husband first lived together in the United States in 2020 .<n>It wasn't until the 44-year-old mother of two moved to Denmark that she became acutely aware she didn‚Äôt use eating utensils like her husband ‚Äî or pretty much any of the Europeans around her <n>Growing up in Illinois, Black says her mother only set their family dinner table with forks, unless there was something being served, such as steak, that warranted a knife to cut it\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"---\n### üîπ Model 3: mT5 (Multilingual T5)\n**mT5 (Multilingual Text-to-Text Transfer Transformer)** is a massive model trained on 101 languages!\n*   **Universal**: It treats every NLP problem as a \"text-to-text\" task.\n*   **Multilingual**: Unlike BART or Pegasus (which are mostly English-focused), mT5 can handle Arabic, French, Chinese, etc.\n*   **XLSum**: We are using a version fine-tuned on the XLSum dataset, covering 45 languages.","metadata":{}},{"cell_type":"markdown","source":"### ‚öôÔ∏è Step 9: Load mT5 Model\nWe load `csebuetnlp/mT5_multilingual_XLSum`.","metadata":{}},{"cell_type":"code","source":"model_name_mt5 = \"csebuetnlp/mT5_multilingual_XLSum\"\ntokenizer_mt5 = AutoTokenizer.from_pretrained(model_name_mt5)\nmodel_mt5 = AutoModelForSeq2SeqLM.from_pretrained(model_name_mt5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:06:52.494164Z","iopub.execute_input":"2026-01-07T16:06:52.494430Z","iopub.status.idle":"2026-01-07T16:07:21.768597Z","shell.execute_reply.started":"2026-01-07T16:06:52.494405Z","shell.execute_reply":"2026-01-07T16:07:21.765840Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d6a19fa11241b88574ef84af31c7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfaad7fbe3ec4e14bf7c25bd138ddb7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da7bd0e7c544454879fea77442705a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c044efdd024003ab65dfa2559a7b84"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f38d6ed0824099b72e96ee800f3426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f733116ba541a89f92a078a9d8dfe3"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### üß™ Step 10: Define mT5 Function\nNotice we allow a slightly longer summary length here (`max_summary_len=250`) as mT5 might be more verbose or handle different languages differently.","metadata":{}},{"cell_type":"code","source":"def summarize_mt5(text, max_input_len=512, max_summary_len=250):\n    import re\n    text = re.sub(r'\\s+', ' ', text.strip())\n    inputs = tokenizer_mt5(text, return_tensors=\"pt\", truncation=True, max_length=max_input_len)\n    output_ids = model_mt5.generate(inputs[\"input_ids\"], max_length=max_summary_len, num_beams=4, no_repeat_ngram_size=2)\n    summary = tokenizer_mt5.decode(output_ids[0], skip_special_tokens=True)\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:07:21.771112Z","iopub.execute_input":"2026-01-07T16:07:21.771463Z","iopub.status.idle":"2026-01-07T16:07:21.782578Z","shell.execute_reply.started":"2026-01-07T16:07:21.771434Z","shell.execute_reply":"2026-01-07T16:07:21.781607Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### üöÄ Step 11: Run mT5 Summarization\nEven on English text, mT5 performs well. Try replacing the `text` variable with **Arabic** text to see its true power!","metadata":{}},{"cell_type":"code","source":"# Example (can use Arabic text too)\nprint(\"üìÑ mT5 Summary:\\n\", summarize_mt5(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:07:21.786244Z","iopub.execute_input":"2026-01-07T16:07:21.786701Z","iopub.status.idle":"2026-01-07T16:07:37.645551Z","shell.execute_reply.started":"2026-01-07T16:07:21.786654Z","shell.execute_reply":"2026-01-07T16:07:37.644608Z"}},"outputs":[{"name":"stdout","text":"üìÑ mT5 Summary:\n When Brooke Black moved to Denmark in 2020, she decided not to use a knife.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T16:07:37.648702Z","iopub.execute_input":"2026-01-07T16:07:37.648978Z","iopub.status.idle":"2026-01-07T16:07:53.157402Z","shell.execute_reply.started":"2026-01-07T16:07:37.648955Z","shell.execute_reply":"2026-01-07T16:07:53.156168Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\nRequirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\nRequirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\nCollecting pydantic<2.12,>=2.0 (from gradio)\n  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\nRequirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nCollecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio)\n  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.41.5\n    Uninstalling pydantic_core-2.41.5:\n      Successfully uninstalled pydantic_core-2.41.5\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.12.5\n    Uninstalling pydantic-2.12.5:\n      Successfully uninstalled pydantic-2.12.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires SQLAlchemy<3,>=1.4, but you have sqlalchemy 1.2.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-2.11.10 pydantic-core-2.33.2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## **7. Models Deployment üöÄ**","metadata":{}},{"cell_type":"markdown","source":"### üöÄ What is Gradio?\n\n**Gradio** is an open-source Python library that allows us to build simple and interactive web interfaces for Machine Learning models with just a few lines of code.\n\nInstead of running models only in the notebook or terminal, Gradio enables users to:\n- Enter text through a web interface ‚úçÔ∏è\n- Run the model interactively ‚öôÔ∏è\n- View predictions instantly üìä\n\n### Why use Gradio?\n- No frontend experience required\n- Very fast to prototype ML demos\n- Perfect for showcasing models to non-technical users\n- Widely used in ML demos and Hugging Face Spaces\n\nIn this notebook, we use Gradio to:\n- Accept user input text\n- Generate summaries using multiple pretrained models\n- Display results clearly in a table format\n","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:11:34.397979Z","iopub.execute_input":"2026-01-07T18:11:34.399128Z","iopub.status.idle":"2026-01-07T18:11:58.987868Z","shell.execute_reply.started":"2026-01-07T18:11:34.399088Z","shell.execute_reply":"2026-01-07T18:11:58.986929Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### ü§ñ Loading Multiple Pretrained Summarization Models\n\nWe load **multiple pretrained text summarization models** from Hugging Face to allow **model comparison**.","metadata":{}},{"cell_type":"code","source":"MODELS = {\n    \"BART (English)\": \"facebook/bart-large-cnn\",\n    \"PEGASUS (English - News)\": \"google/pegasus-cnn_dailymail\",\n    \"mT5 (Multilingual)\": \"csebuetnlp/mT5_multilingual_XLSum\"\n}\n\ntokenizers = {}\nmodels = {}\n\nfor name, path in MODELS.items():\n    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n    models[name] = AutoModelForSeq2SeqLM.from_pretrained(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:11:58.989413Z","iopub.execute_input":"2026-01-07T18:11:58.990140Z","iopub.status.idle":"2026-01-07T18:13:55.662203Z","shell.execute_reply.started":"2026-01-07T18:11:58.990100Z","shell.execute_reply":"2026-01-07T18:13:55.660397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0cf09c632d54ba9908e9dd5c564b6fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e4a1b62b2646a299548293604c7c17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99a0dd579424f958c108e8d4060b6c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39abd3831ed14fb0ab7dcb945fbe68a1"}},"metadata":{}},{"name":"stderr","text":"2026-01-07 18:12:05.898456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767809526.203532      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767809526.293861      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767809527.073508      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767809527.073565      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767809527.073569      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767809527.073571      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d77077303f469485a7dcf7d9cde398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65bf5d92f9f4294b61c1c9eb248e7f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c1bd96b6534c09987c0bb77bfa8387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af1afd41ecf42e79a8becb76b3faf66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18756239585425b90605a7d20517323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c46f9da2d74504805d84cdf3bf9f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9f8d83316e40bb824c40bd4372627e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99dcb763febc42f7aad1e9a928272a4b"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcfae11a1f1f4a399bd91a2dd38e79d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc95a78ace447f3b8e8237e72a578c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef3bcfe484049448ca47b5b62109faf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08520aa0a19c422f96ff3437fe08ccd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d65e4304f7f471392c334a9e66079b9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"493fa3eb7f774c1885ee612e8ef0b6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e098dec4a2e44af9f482730768d799e"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### üìä Creating a Results DataFrame\n\nWe create an **empty DataFrame** to store all summarization results generated by different models.","metadata":{}},{"cell_type":"code","source":"results_df = pd.DataFrame(\n    columns=[\"Timestamp\", \"Model\", \"Summary\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:13:55.664718Z","iopub.execute_input":"2026-01-07T18:13:55.667480Z","iopub.status.idle":"2026-01-07T18:13:55.716056Z","shell.execute_reply.started":"2026-01-07T18:13:55.667373Z","shell.execute_reply":"2026-01-07T18:13:55.714481Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### üß† Summarization and Result Storage Function\n\nWe define the core function that powers our application:\n**`summarize_and_store()`**\n\nThis function is responsible for:\n- Generating a summary using a selected pretrained model\n- Cleaning and post-processing the output\n- Saving the result in a shared DataFrame\n- Returning both the summary and the updated table to the user\n","metadata":{}},{"cell_type":"code","source":"def summarize_and_store(text, model_name):\n    global results_df\n\n    if not text.strip():\n        return \"‚ö†Ô∏è Please enter some text.\", results_df\n\n    text_clean = re.sub(r'\\s+', ' ', text.strip())\n\n    tokenizer = tokenizers[model_name]\n    model = models[model_name]\n\n    inputs = tokenizer(\n        text_clean,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512\n    )\n\n    output_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=100,\n        num_beams=4,\n        no_repeat_ngram_size=2\n    )\n\n    summary = tokenizer.decode(\n        output_ids[0],\n        skip_special_tokens=True\n    )\n\n    summary = re.sub(r'<n>', ' ', summary)\n    summary = re.sub(r'\\s+', ' ', summary).strip()\n\n    new_row = {\n        \"Timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n        \"Model\": model_name,\n        \"Summary\": summary\n    }\n\n    results_df = pd.concat(\n        [results_df, pd.DataFrame([new_row])],\n        ignore_index=True\n    )\n\n    return summary, results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:13:55.725321Z","iopub.execute_input":"2026-01-07T18:13:55.725983Z","iopub.status.idle":"2026-01-07T18:13:55.761703Z","shell.execute_reply.started":"2026-01-07T18:13:55.725828Z","shell.execute_reply":"2026-01-07T18:13:55.756437Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### üñ•Ô∏è Building the Gradio User Interface\n\nIn this cell, we create an **interactive web interface** using **Gradio**","metadata":{}},{"cell_type":"code","source":"interface = gr.Interface(\n    fn=summarize_and_store,\n    inputs=[\n        gr.Textbox(\n            lines=10,\n            placeholder=\"‚úçÔ∏è Paste your text here...\",\n            label=\"Input Text\"\n        ),\n        gr.Dropdown(\n            choices=list(MODELS.keys()),\n            value=\"PEGASUS (English - News)\",\n            label=\"Choose Model\"\n        )\n    ],\n    outputs=[\n        gr.Textbox(\n            lines=4,\n            label=\"üìÑ Generated Summary\"\n        ),\n        gr.Dataframe(\n            label=\"üìä All Generated Summaries\",\n            interactive=False\n        )\n    ],\n    title=\"üìù Text Summarization with Pretrained Models\",\n    description=(\n        \"Generate summaries using different pretrained models.\\n\\n\"\n        \"‚¨áÔ∏è All results appear in the table below for easy comparison.\"\n    )\n)\n\ninterface.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T18:13:55.764709Z","iopub.execute_input":"2026-01-07T18:13:55.767869Z","iopub.status.idle":"2026-01-07T18:14:00.326437Z","shell.execute_reply.started":"2026-01-07T18:13:55.767803Z","shell.execute_reply":"2026-01-07T18:14:00.325491Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://d968d8a369af17eccd.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d968d8a369af17eccd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"---\n## 8. Conclusion üéì\n\nCongratulations! You have successfully built a text summarization pipeline using three different Transformer models.\n\n**Key Takeaways:**\n*   **Hugging Face** makes it easy to swap between powerful models with just a few lines of code.\n*   **Abstractive Summarization** creates human-like summaries by rewriting text.\n*   **Different Models, Different Flavors**: BART is great for general English, Pegasus is specialized for news, and mT5 is the go-to for multilingual tasks.\n\n**Next Steps:**\n*   **Try your own text**: Copy a news article or a paragraph from a book.\n*   **Evaluation**: Learn about **ROUGE scores** to mathematically measure how good a summary is.\n*   **Fine-Tuning**: Train these models on your own specific dataset (e.g., medical reports, legal assumptions).\n","metadata":{}}]}